{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\natha\\anaconda3\\lib\\site-packages (1.5.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\natha\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\natha\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm, pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "drop_list = [\"handle\", \"id\", \"retweet_count\", \"reply_count\", \"interaction_score\", \"quote_count\", \"followers\", \"text_first_hashtag\", \"url_image\", \"text\"]\n",
    "LABEL = \"like_count\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, drop=[]):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(url)\n",
    "    if len(drop) > 0:\n",
    "        for col in drop:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df\n",
    "\n",
    "def bin_groups(df, percent=.05):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            for group, count in df[col].value_counts().iteritems():\n",
    "                if count / len(df) < percent:\n",
    "                    df.loc[df[col] == group, col] = 'Other'\n",
    "    return df\n",
    "\n",
    "def drop_columns_missing_data(df, cutoff=.5):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if df[col].isna().sum() / len(df) > cutoff:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df\n",
    "\n",
    "def impute_mean(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import pandas as pd, numpy as np\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "def impute_KNN(df):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns = df.columns)\n",
    "    imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "        \n",
    "def impute_reg(df):\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = IterativeImputer(max_iter=10, random_state=12345)\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "def fs_variance(df, label=\"\", p=0.8):\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    import pandas as pd\n",
    "\n",
    "    if label != \"\":\n",
    "        X = df.drop(columns=[label])\n",
    "    \n",
    "    sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "    sel.fit(X)\n",
    "\n",
    "    # Add the label back in after removing poor features\n",
    "    return df[df.columns[sel.get_support(indices=True)]].join(df[label])\n",
    "\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    from numpy import mean, std\n",
    "    X = df.drop(label,axis=1)\n",
    "    y = df[label]\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "    scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "    print(f'Average R-squared:\\t{mean(scores)}')\n",
    "    return LinearRegression().fit(X, y)\n",
    "\n",
    "def dump_pickle(model, file_name):\n",
    "    import pickle\n",
    "    pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    import pickle\n",
    "    model = pickle.load(open(file_name, \"rb\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_crossvalidate_clf(df, label, k=10, r=5, repeat=True):\n",
    "    import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se, numpy as np\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    from numpy import mean, std\n",
    "    from sklearn import svm\n",
    "    from sklearn import gaussian_process\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "    from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "    from sklearn import svm\n",
    "    from sklearn.naive_bayes import CategoricalNB\n",
    "    from xgboost import XGBClassifier\n",
    "    from sklearn import preprocessing\n",
    "    from sklearn.neural_network import MLPClassifier\n",
    "    \n",
    "    X = df.drop(columns=[label])\n",
    "    y = df[label]\n",
    "    \n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "    \n",
    "    fit = {}    # Use this to store each of the fit metrics\n",
    "    models = {} # Use this to store each of the models\n",
    "    \n",
    "    # Create the model objects\n",
    "    model_log = lm.LogisticRegression(max_iter=100)\n",
    "    model_logcv = lm.RidgeClassifier()\n",
    "    model_sgd = lm.SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    model_pa = lm.PassiveAggressiveClassifier(max_iter=1000, random_state=12345, tol=1e-3)\n",
    "    model_per = lm.Perceptron(fit_intercept=False, max_iter=10, tol=None, shuffle=False)\n",
    "    model_knn = KNeighborsClassifier(n_neighbors=3)\n",
    "    model_svm = svm.SVC(decision_function_shape='ovo') # Remove the parameter for two-class model\n",
    "    model_nb = CategoricalNB()\n",
    "    model_bag = se.BaggingClassifier(KNeighborsClassifier(), max_samples=0.5, max_features=0.5)\n",
    "    model_ada = se.AdaBoostClassifier(n_estimators=100, random_state=12345)\n",
    "    model_ext = se.ExtraTreesClassifier(n_estimators=100, random_state=12345)\n",
    "    model_rf = se.RandomForestClassifier(n_estimators=10)\n",
    "    model_hgb = se.HistGradientBoostingClassifier(max_iter=100)\n",
    "    model_vot = se.VotingClassifier(estimators=[('lr', model_log), ('rf', model_ext), ('gnb', model_hgb)], voting='hard')\n",
    "    model_gb = se.GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0)\n",
    "    estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=12345)), ('knr', KNeighborsRegressor(n_neighbors=20, metric='euclidean'))]\n",
    "    final_estimator = se.GradientBoostingRegressor(n_estimators=25, subsample=0.5, min_samples_leaf=25, max_features=1, random_state=12345)\n",
    "    model_st = se.StackingRegressor(estimators=estimators, final_estimator=final_estimator)\n",
    "    model_xgb = XGBClassifier()\n",
    "    model_nn = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5, 2), random_state=12345)\n",
    "    \n",
    "    # Fit a crss-validated R squared score and add it to the dict\n",
    "    fit['Logistic'] = mean(cross_val_score(model_log, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Ridge'] = mean(cross_val_score(model_logcv, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['SGD'] = mean(cross_val_score(model_sgd, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['PassiveAggressive'] = mean(cross_val_score(model_pa, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Perceptron'] = mean(cross_val_score(model_per, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['KNN'] = mean(cross_val_score(model_knn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['SVM'] = mean(cross_val_score(model_svm, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['NaiveBayes'] = mean(cross_val_score(model_nb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Bagging'] = mean(cross_val_score(model_bag, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['AdaBoost'] = mean(cross_val_score(model_ada, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['ExtraTrees'] = mean(cross_val_score(model_ext, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['RandomForest'] = mean(cross_val_score(model_rf, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['HistGradient'] = mean(cross_val_score(model_hgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['Voting'] = mean(cross_val_score(model_vot, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['GradBoost'] = mean(cross_val_score(model_gb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    fit['NeuralN'] = mean(cross_val_score(model_nn, X, y, scoring='accuracy', cv=cv, n_jobs=-1))\n",
    "    \n",
    "    # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "    models['Logistic'] = model_log\n",
    "    models['Ridge'] = model_logcv\n",
    "    models['SGD'] = model_sgd\n",
    "    models['PassiveAggressive'] = model_pa\n",
    "    models['Perceptron'] = model_per\n",
    "    models['KNN'] = model_knn\n",
    "    models['SVM'] = model_svm\n",
    "    models['NaiveBayes'] = model_nb\n",
    "    models['Bagging'] = model_bag\n",
    "    models['AdaBoost'] = model_ada\n",
    "    models['ExtraTrees'] = model_ext\n",
    "    models['RandomForest'] = model_rf\n",
    "    models['HistGradient'] = model_hgb\n",
    "    models['Voting'] = model_vot\n",
    "    models['GradBoost'] = model_gb\n",
    "    models['XGBoost'] = model_xgb\n",
    "    models['NeuralN'] = model_nn\n",
    "    \n",
    "        # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "    df_fit = pd.DataFrame({'Accuracy':fit})\n",
    "    df_fit.sort_values(by=['Accuracy'], ascending=False, inplace=True)\n",
    "    best_model = df_fit.index[0]\n",
    "    print(df_fit)\n",
    "    \n",
    "    return models[best_model].fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20900/803466644.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Feature selection and modeling pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfs_variance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mLABEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_crossvalidate_clf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLABEL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Deployment pipeline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20900/610675280.py\u001b[0m in \u001b[0;36mfit_crossvalidate_clf\u001b[1;34m(df, label, k, r, repeat)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mmodel_ext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExtraTreesClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m12345\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[0mmodel_rf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m     \u001b[0mmodel_hgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mHistGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mmodel_vot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVotingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_log\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'rf'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_ext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'gnb'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_hgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'hard'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[0mmodel_gb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'sklearn.ensemble' has no attribute 'HistGradientBoostingClassifier'"
     ]
    }
   ],
   "source": [
    "# Data cleaning and preparation pipeline\n",
    "df = get_data('noRepies-topics.csv').drop(columns=drop_list)\n",
    "df = bin_groups(df)\n",
    "df = drop_columns_missing_data(df)\n",
    "\n",
    "# Drop the label so it does not get dummy coded, then join it back in after\n",
    "df = impute_mean(df.drop(columns=[LABEL])).join(df[LABEL])\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df = fs_variance(df, label=LABEL, p=.5)\n",
    "model = fit_crossvalidate_clf(df, LABEL, 5, 2)\n",
    "\n",
    "# Deployment pipeline\n",
    "dump_pickle(model, 'best_clf_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d835b9d6547198496337f4d2de04abf3395832a8b4aee7b55cf3102d3ef3dae9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
