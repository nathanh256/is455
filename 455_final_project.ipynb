{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "UekkzUwDavf2"
      },
      "outputs": [],
      "source": [
        "# ### Mount Drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "HfSN7hb4aphV"
      },
      "outputs": [],
      "source": [
        "### Imports\n",
        "\n",
        "import os, requests, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import statsmodels.api as sm\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "import seaborn as sns\n",
        "from sklearn import preprocessing\n",
        "import sys"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [],
      "source": [
        "### Constants\n",
        "\n",
        "WEIGHTS = {\"retweet\" : 3, \"like\" : 0.5 ,\"quote\" : 4 ,\"reply\" : 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "2k1VI619aNhd"
      },
      "outputs": [],
      "source": [
        "bearer_token = \"AAAAAAAAAAAAAAAAAAAAAGXdTwEAAAAAr2%2BC9Wi6GHR8%2Bk%2FiDL2AIHaC1I8%3D86fg9nIXAt2MFp0QP1sXU0q1VFKHAGaD1da68qG4X0glvGSh4D\"\n",
        "\n",
        "def response_health(r):\n",
        "  if r.status_code != 200:\n",
        "    raise Exception(\n",
        "    \"Request returned an error: {} {}\".format(\n",
        "      r.status_code, r.text\n",
        "    )\n",
        "  )\n",
        "    \n",
        "def bearer_oauth(r):\n",
        "  r.headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n",
        "  return r\n",
        "\n",
        "def send_request(url, params=None, print_status=False):\n",
        "  '''Send Request (url) with optional params. Returns json'''\n",
        "  # https://2.python-requests.org/en/master/api/#requests.request\n",
        "  if params == None:\n",
        "    response = requests.request(\"GET\", url, auth=bearer_oauth)\n",
        "  else:\n",
        "    response = requests.request(\"GET\", url, auth=bearer_oauth, params=params)\n",
        "  if print_status: print(\"Request response status: \", response.status_code)\n",
        "  response_health(response)\n",
        "  return response.json()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "VvapD2RKjCuA"
      },
      "outputs": [],
      "source": [
        "def get_user_data(name):\n",
        "  # data dictionary scroll down to response fields https://developer.twitter.com/en/docs/twitter-api/users/lookup/api-reference/get-users-by-username-username\n",
        "\n",
        "  userFields = {\"user.fields\":\"created_at, description, entities, id, location, name, pinned_tweet_id, profile_image_url, protected, public_metrics, url, username, verified, withheld\".replace(\" \", \"\")}\n",
        "  user_json = send_request(f\"https://api.twitter.com/2/users/by/username/{name}\",params=userFields)\n",
        "  user_json = user_json[\"data\"]\n",
        "\n",
        "  outputDict = {}\n",
        "  outputDict['following_count'] = user_json['public_metrics']['following_count']\n",
        "  outputDict['tweet_count'] = user_json['public_metrics']['tweet_count']\n",
        "  outputDict['followers_count'] = user_json['public_metrics']['followers_count']\n",
        "  outputDict['listed_count'] = user_json['public_metrics']['listed_count']\n",
        "  outputDict['username'] = user_json['username']\n",
        "  outputDict['name'] = user_json['name']\n",
        "  outputDict['id'] = user_json['id']\n",
        "  outputDict['verified'] = user_json['verified']\n",
        "  outputDict['protected'] = user_json['protected']\n",
        "  outputDict['created_at'] = user_json['created_at']\n",
        "  outputDict['description'] = user_json['description']\n",
        "\n",
        "  try:\n",
        "    test = user_json['pinned_tweet_id']\n",
        "    outputDict['hasPinnedTweet'] = True\n",
        "  except:\n",
        "    outputDict['hasPinnedTweet'] = False\n",
        "    pass\n",
        "  try:\n",
        "    outputDict['urlsInDescription'] = len(user_json['entities']['description']['urls'])\n",
        "  except:\n",
        "    outputDict['urlsInDescription'] = 0\n",
        "    pass\n",
        "  try:\n",
        "    outputDict['hashtagsInDescription'] = len(user_json['entities']['description']['hashtags'])\n",
        "  except:\n",
        "    outputDict['hashtagsInDescription'] = 0\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    outputDict['userWebsitesAdded'] = len(user_json['entities']['url']['urls'])\n",
        "  except:\n",
        "    outputDict['userWebsitesAdded'] = 0\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    outputDict['cashtagsInDescription'] = len(user_json['entities']['description']['cashtags'])\n",
        "  except:\n",
        "    outputDict['cashtagsInDescription'] = 0\n",
        "    pass\n",
        "\n",
        "  try:\n",
        "    outputDict['mentionsInDescription'] = len(user_json['entities']['description']['mentions'])\n",
        "  except:\n",
        "    outputDict['mentionsInDescription'] = 0\n",
        "    pass\n",
        "\n",
        "  \n",
        "  return outputDict"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "cBRiEBATn6Kc"
      },
      "outputs": [],
      "source": [
        "def get_tweets_user(id, numTweets = 10, tweetsPerPage = 10, replies = False, weights=WEIGHTS):\n",
        "    import math\n",
        "    \n",
        "    # (there are 10 results returned per page by default)\n",
        "    if numTweets < tweetsPerPage:\n",
        "        print(\"numTweets must be greater than or equal to the number of tweets per page.\")\n",
        "        return\n",
        "\n",
        "    # to see data dictionary, click url and scroll down to response fields https://developer.twitter.com/en/docs/twitter-api/tweets/timelines/api-reference/get-users-id-tweets\n",
        "    expansions = {\"expansions\":\"author_id, attachments.poll_ids, attachments.media_keys, entities.mentions.username, geo.place_id, in_reply_to_user_id, referenced_tweets.id,referenced_tweets.id.author_id\".replace(\" \", \"\")}\n",
        "    tweetFields = {\"tweet.fields\":\"attachments, author_id, context_annotations, conversation_id, created_at, entities, geo, id, in_reply_to_user_id, lang, public_metrics, possibly_sensitive, referenced_tweets, reply_settings, source, text, withheld\".replace(\" \", \"\")}\n",
        "    userFields = {\"user.fields\":\"public_metrics,username\"}\n",
        "    replyFields = {\"exclude\": \"replies\"}\n",
        "\n",
        "    outputDict = {'id':[],'handle':[],'followers':[], 'text':[], 'lang':[],'possibly_sensitive':[],'retweet_count':[],'reply_count':[],'like_count':[],'quote_count':[]\n",
        "        ,'reply_settings':[],'source':[],'created_at':[],'is_retweet':[],'contains_quote':[],'is_reply':[],'num_referenced_tweets':[],\n",
        "        'url_image':[],'num_hashtags':[],'text_first_hashtag':[],'num_mentions':[],'num_cashtags':[],'num_polls':[]}\n",
        "\n",
        "    if not replies:\n",
        "        outputDict['interaction_score'] = []\n",
        "\n",
        "    # for each page of results\n",
        "    for i in range(math.ceil(numTweets/tweetsPerPage)): \n",
        "        if i != 0:\n",
        "            if replies:\n",
        "                params = {**{'max_results':tweetsPerPage}, **expansions,**tweetFields,**userFields , **{\"pagination_token\":nextToken}}\n",
        "            else:\n",
        "                params = {**{'max_results':tweetsPerPage}, **expansions,**tweetFields,**userFields , **{\"pagination_token\":nextToken}, **replyFields}\n",
        "        else:\n",
        "            if replies:\n",
        "                params = {**{'max_results':tweetsPerPage}, **expansions,**tweetFields, **userFields}\n",
        "            else:\n",
        "                params = {**{'max_results':tweetsPerPage}, **expansions,**tweetFields, **userFields, **replyFields}\n",
        "\n",
        "\n",
        "        tweet_json = (send_request(f\"https://api.twitter.com/2/users/{id}/tweets\", params=params))\n",
        "        \n",
        "        tweetData = tweet_json['data']\n",
        "\n",
        "        username = tweet_json['includes']['users'][0]['username']\n",
        "        followers = tweet_json['includes']['users'][0]['public_metrics']['followers_count']\n",
        "\n",
        "        for tweet in tweetData:\n",
        "            outputDict['handle'].append(username)\n",
        "            outputDict['followers'].append(followers)\n",
        "            outputDict['id'].append(tweet['id'])\n",
        "            outputDict['text'].append(tweet['text'])\n",
        "            outputDict['lang'].append(tweet['lang'])\n",
        "            outputDict['possibly_sensitive'].append(tweet['possibly_sensitive'])\n",
        "\n",
        "            outputDict['retweet_count'].append(tweet['public_metrics']['retweet_count'])\n",
        "            outputDict['reply_count'].append(tweet['public_metrics']['reply_count'])\n",
        "            outputDict['like_count'].append(tweet['public_metrics']['like_count'])\n",
        "            outputDict['quote_count'].append(tweet['public_metrics']['quote_count'])\n",
        "            outputDict['reply_settings'].append(tweet['reply_settings'])\n",
        "            outputDict['source'].append(tweet['source'])\n",
        "            outputDict['created_at'].append(tweet['created_at'])\n",
        "\n",
        "            if not replies:\n",
        "                outputDict['interaction_score'].append((weights['retweet'] *outputDict['retweet_count'][-1] + weights['like'] *outputDict['like_count'][-1]+\n",
        "                                                        weights['reply'] *outputDict['reply_count'][-1] + weights['quote'] *outputDict['quote_count'][-1])/followers)\n",
        "\n",
        "\n",
        "            # referenced tweets: quotes, replies, and retweets\n",
        "            try:\n",
        "                refdTweets = tweet['referenced_tweets']\n",
        "                outputDict['num_referenced_tweets'].append(len(refdTweets))\n",
        "\n",
        "                rtweet = False\n",
        "                reply = False\n",
        "                quote = False\n",
        "                # there may be multiple referenced tweets, apparently. So it could be a reply and contain a quote, I guess\n",
        "                for t in refdTweets:\n",
        "                    typ = t['type']\n",
        "                    if typ == 'retweeted':\n",
        "                        outputDict['is_retweet'].append(True)\n",
        "                        rtweet = True\n",
        "                    elif typ == 'quoted':\n",
        "                        outputDict['contains_quote'].append(True)\n",
        "                        quote = True\n",
        "                    elif typ == 'replied_to':\n",
        "                        outputDict['is_reply'].append(True)\n",
        "                        reply = True\n",
        "                        \n",
        "                if not rtweet:\n",
        "                        outputDict['is_retweet'].append(False)\n",
        "                if not reply:\n",
        "                        outputDict['is_reply'].append(False)\n",
        "                if not quote:\n",
        "                        outputDict['contains_quote'].append(False)\n",
        "\n",
        "            except:\n",
        "                outputDict['num_referenced_tweets'].append(0)\n",
        "                outputDict['is_retweet'].append(False)\n",
        "                outputDict['contains_quote'].append(False)\n",
        "                outputDict['is_reply'].append(False)\n",
        "                pass\n",
        "\n",
        "\n",
        "            # image\n",
        "            try:\n",
        "                outputDict['url_image'].append(tweet['entities']['urls'][0]['images'][0]['url'])  #just grabbing the first image in the first url\n",
        "            except:\n",
        "                outputDict['url_image'].append(\"\")  \n",
        "                pass\n",
        "\n",
        "            # hashtags\n",
        "            try:\n",
        "                outputDict['num_hashtags'].append(len(tweet['entities']['hashtags']))\n",
        "                # grabbing just the first hashtag\n",
        "                outputDict['text_first_hashtag'].append(tweet['entities']['hashtags'][0]['tag'])\n",
        "            except:\n",
        "                outputDict['num_hashtags'].append(0) \n",
        "                outputDict['text_first_hashtag'].append(\"\")   \n",
        "                pass\n",
        "\n",
        "            # mentions\n",
        "            try:\n",
        "                outputDict['num_mentions'].append(len(tweet['entities']['mentions']))\n",
        "            except:\n",
        "                outputDict['num_mentions'].append(0) \n",
        "                pass\n",
        "\n",
        "            # cashtags\n",
        "            try:\n",
        "                outputDict['num_cashtags'].append(len(tweet['entities']['cashtags']))\n",
        "            except:\n",
        "                outputDict['num_cashtags'].append(0) \n",
        "                pass\n",
        "            \n",
        "            # polls\n",
        "            try:\n",
        "                outputDict['num_polls'].append(len(tweet['attachments']['poll_ids']))\n",
        "            except:\n",
        "                outputDict['num_polls'].append(0) \n",
        "                pass\n",
        "\n",
        "\n",
        "        nextToken = tweet_json['meta']['next_token']\n",
        "    \n",
        "    \n",
        "    df = pd.DataFrame(outputDict)\n",
        "\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "qwGZb0eTnvrM"
      },
      "outputs": [],
      "source": [
        "def get_api_data(usernames, replies = False, numTweets = 10, tweetsPerPage = 10, print_status = False):\n",
        "    outputDict = {}\n",
        "    usersDict =  {'following_count':[], 'tweet_count':[], 'followers_count':[], 'listed_count':[], 'username':[], 'name':[], 'id':[], \n",
        "    'verified': [], 'protected': [],'created_at': [],'description': [], 'hasPinnedTweet':[], 'urlsInDescription':[], 'hashtagsInDescription':[],\n",
        "    'userWebsitesAdded':[], 'cashtagsInDescription':[],'mentionsInDescription':[]}\n",
        "\n",
        "    df_tweets = None\n",
        "    for iteration, username in enumerate(usernames):\n",
        "        # get data related to user account\n",
        "        userData = get_user_data(username)\n",
        "        for k, v in userData.items():\n",
        "            usersDict[k].append(v)\n",
        "\n",
        "        # get data from tweets of the user\n",
        "        df_sub = get_tweets_user(userData['id'], numTweets=numTweets, tweetsPerPage=tweetsPerPage, replies=replies)\n",
        "        if iteration != 0:\n",
        "            df_tweets = pd.concat([df_tweets, df_sub])\n",
        "        else:\n",
        "            df_tweets = df_sub\n",
        "        if print_status: \n",
        "            num_tweets = len(df_sub)\n",
        "            print(f\"Retrieved {num_tweets} tweets for: {username}\")\n",
        "    \n",
        "    df_user = pd.DataFrame(usersDict)\n",
        "\n",
        "    return df_tweets, df_user\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['Schlotzskys',\n",
              " 'AuntieAnnes',\n",
              " 'SaltgrassSteak',\n",
              " 'redlobster',\n",
              " 'Hardees',\n",
              " 'RuthsChris',\n",
              " 'LongHornSteaks',\n",
              " 'FiveGuys',\n",
              " 'Applebees',\n",
              " 'DelTaco',\n",
              " 'PFChangs',\n",
              " 'BonefishGrill',\n",
              " 'Charleys',\n",
              " 'EinsteinBros',\n",
              " 'qdoba',\n",
              " 'torchystacos',\n",
              " 'raisingcanes',\n",
              " 'Cheesecake',\n",
              " 'WaffleHouse',\n",
              " 'CheckersRallys',\n",
              " 'SmoothieKing',\n",
              " 'CaptainDs',\n",
              " 'WhiteCastle',\n",
              " 'papamurphys',\n",
              " 'caferio',\n",
              " 'ChuysRestaurant',\n",
              " 'ChurchsChicken',\n",
              " 'IHOP',\n",
              " 'BaskinRobbins',\n",
              " 'TSmoothieCafe',\n",
              " 'MODPizza',\n",
              " 'calpizzakitchen',\n",
              " 'FreddysUSA',\n",
              " 'SteaknShake',\n",
              " 'tacojohns',\n",
              " 'Dickeys',\n",
              " 'krispykreme',\n",
              " 'ElPolloLoco',\n",
              " 'ColdStone',\n",
              " 'Whataburger',\n",
              " 'Hooters',\n",
              " 'Maggianos',\n",
              " 'hungryhowies',\n",
              " 'noodlescompany',\n",
              " 'Carrabbas',\n",
              " 'shakeshack',\n",
              " 'jimmyjohns',\n",
              " 'portilloshotdog',\n",
              " 'culvers',\n",
              " 'redrobinburgers',\n",
              " 'goldencorral',\n",
              " 'eatatjacks',\n",
              " 'McAlistersDeli',\n",
              " 'rubytuesday',\n",
              " 'BobEvansFarms',\n",
              " 'CHWinery',\n",
              " 'jasonsdeli',\n",
              " 'longjohnsilvers',\n",
              " 'TGIFridays',\n",
              " 'Potbelly',\n",
              " 'wingstop',\n",
              " 'JambaJuice',\n",
              " 'cheddarskitchen',\n",
              " 'CapitalGrille',\n",
              " 'EatAtPerkins',\n",
              " 'MillersAleHouse',\n",
              " 'JetsPizza',\n",
              " 'ZoesKitchen',\n",
              " 'peetscoffee',\n",
              " 'BlazePizza',\n",
              " 'bostonmarket',\n",
              " 'OCharleys',\n",
              " 'jerseymikes',\n",
              " 'bjsrestaurants',\n",
              " 'Bojangles',\n",
              " 'TimHortons',\n",
              " 'DennysDiner',\n",
              " 'Chilis',\n",
              " 'Outback',\n",
              " 'Moes_HQ',\n",
              " 'FirehouseSubs',\n",
              " 'DutchBros',\n",
              " 'RoundTablePizza',\n",
              " 'pollotropical',\n",
              " 'YardHouse',\n",
              " 'MellowMushroom',\n",
              " 'texasroadhouse',\n",
              " 'olivegarden',\n",
              " 'CrackerBarrel',\n",
              " 'PapaJohns',\n",
              " 'habitburger',\n",
              " 'Zaxbys',\n",
              " 'BWWings',\n",
              " 'MarcosPizza',\n",
              " 'McDonalds',\n",
              " 'CarlsJr']"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df = pd.read_csv('fast-food-chains - Sheet1.csv')\n",
        "\n",
        "df.drop(df.columns[0], axis=1, inplace=True)\n",
        "df.loc[df.shape[0]] = df.columns\n",
        "df.columns = [\"handle\"]\n",
        "twitter_handles = df[df.columns[0]].tolist()\n",
        "twitter_handles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Retrieved 80 tweets for: Schlotzskys\n",
            "Retrieved 80 tweets for: AuntieAnnes\n",
            "Retrieved 80 tweets for: SaltgrassSteak\n",
            "Retrieved 80 tweets for: redlobster\n",
            "Retrieved 80 tweets for: Hardees\n",
            "Retrieved 80 tweets for: RuthsChris\n",
            "Retrieved 80 tweets for: LongHornSteaks\n",
            "Retrieved 80 tweets for: FiveGuys\n",
            "Retrieved 80 tweets for: Applebees\n",
            "Retrieved 80 tweets for: DelTaco\n",
            "Retrieved 80 tweets for: PFChangs\n",
            "Retrieved 80 tweets for: BonefishGrill\n",
            "Retrieved 80 tweets for: Charleys\n",
            "Retrieved 80 tweets for: EinsteinBros\n",
            "Retrieved 80 tweets for: qdoba\n",
            "Retrieved 80 tweets for: torchystacos\n",
            "Retrieved 80 tweets for: raisingcanes\n",
            "Retrieved 80 tweets for: Cheesecake\n",
            "Retrieved 80 tweets for: WaffleHouse\n",
            "Retrieved 80 tweets for: CheckersRallys\n",
            "Retrieved 80 tweets for: SmoothieKing\n",
            "Retrieved 80 tweets for: CaptainDs\n",
            "Retrieved 80 tweets for: WhiteCastle\n",
            "Retrieved 80 tweets for: papamurphys\n",
            "Retrieved 80 tweets for: caferio\n",
            "Retrieved 80 tweets for: ChuysRestaurant\n",
            "Retrieved 80 tweets for: ChurchsChicken\n",
            "Retrieved 80 tweets for: IHOP\n",
            "Retrieved 80 tweets for: BaskinRobbins\n",
            "Retrieved 80 tweets for: TSmoothieCafe\n",
            "Retrieved 80 tweets for: MODPizza\n",
            "Retrieved 80 tweets for: calpizzakitchen\n",
            "Retrieved 80 tweets for: FreddysUSA\n",
            "Retrieved 80 tweets for: SteaknShake\n",
            "Retrieved 80 tweets for: tacojohns\n",
            "Retrieved 80 tweets for: Dickeys\n",
            "Retrieved 80 tweets for: krispykreme\n",
            "Retrieved 80 tweets for: ElPolloLoco\n",
            "Retrieved 80 tweets for: ColdStone\n",
            "Retrieved 80 tweets for: Whataburger\n",
            "Retrieved 80 tweets for: Hooters\n",
            "Retrieved 80 tweets for: Maggianos\n",
            "Retrieved 80 tweets for: hungryhowies\n",
            "Retrieved 80 tweets for: noodlescompany\n",
            "Retrieved 80 tweets for: Carrabbas\n",
            "Retrieved 80 tweets for: shakeshack\n",
            "Retrieved 80 tweets for: jimmyjohns\n",
            "Retrieved 80 tweets for: portilloshotdog\n",
            "Retrieved 80 tweets for: culvers\n",
            "Retrieved 80 tweets for: redrobinburgers\n",
            "Retrieved 80 tweets for: goldencorral\n",
            "Retrieved 80 tweets for: eatatjacks\n",
            "Retrieved 80 tweets for: McAlistersDeli\n",
            "Retrieved 80 tweets for: rubytuesday\n",
            "Retrieved 80 tweets for: BobEvansFarms\n",
            "Retrieved 80 tweets for: CHWinery\n",
            "Retrieved 80 tweets for: jasonsdeli\n",
            "Retrieved 80 tweets for: longjohnsilvers\n",
            "Retrieved 80 tweets for: TGIFridays\n",
            "Retrieved 80 tweets for: Potbelly\n",
            "Retrieved 80 tweets for: wingstop\n",
            "Retrieved 80 tweets for: JambaJuice\n",
            "Retrieved 80 tweets for: cheddarskitchen\n",
            "Retrieved 80 tweets for: CapitalGrille\n",
            "Retrieved 80 tweets for: EatAtPerkins\n",
            "Retrieved 80 tweets for: MillersAleHouse\n",
            "Retrieved 80 tweets for: JetsPizza\n",
            "Retrieved 79 tweets for: ZoesKitchen\n",
            "Retrieved 80 tweets for: peetscoffee\n",
            "Retrieved 80 tweets for: BlazePizza\n",
            "Retrieved 79 tweets for: bostonmarket\n",
            "Retrieved 80 tweets for: OCharleys\n",
            "Retrieved 80 tweets for: jerseymikes\n",
            "Retrieved 80 tweets for: bjsrestaurants\n",
            "Retrieved 80 tweets for: Bojangles\n",
            "Retrieved 80 tweets for: TimHortons\n",
            "Retrieved 80 tweets for: DennysDiner\n",
            "Retrieved 80 tweets for: Chilis\n",
            "Retrieved 80 tweets for: Outback\n",
            "Retrieved 80 tweets for: Moes_HQ\n",
            "Retrieved 80 tweets for: FirehouseSubs\n",
            "Retrieved 80 tweets for: DutchBros\n",
            "Retrieved 80 tweets for: RoundTablePizza\n",
            "Retrieved 80 tweets for: pollotropical\n",
            "Retrieved 80 tweets for: YardHouse\n",
            "Retrieved 80 tweets for: MellowMushroom\n",
            "Retrieved 80 tweets for: texasroadhouse\n",
            "Retrieved 80 tweets for: olivegarden\n",
            "Retrieved 80 tweets for: CrackerBarrel\n",
            "Retrieved 80 tweets for: PapaJohns\n",
            "Retrieved 80 tweets for: habitburger\n",
            "Retrieved 80 tweets for: Zaxbys\n",
            "Retrieved 80 tweets for: BWWings\n",
            "Retrieved 80 tweets for: MarcosPizza\n",
            "Retrieved 80 tweets for: McDonalds\n",
            "Retrieved 80 tweets for: CarlsJr\n",
            "Retrieved 120 tweets for: Schlotzskys\n",
            "Retrieved 120 tweets for: AuntieAnnes\n",
            "Retrieved 120 tweets for: SaltgrassSteak\n",
            "Retrieved 120 tweets for: redlobster\n",
            "Retrieved 120 tweets for: Hardees\n",
            "Retrieved 120 tweets for: RuthsChris\n",
            "Retrieved 120 tweets for: LongHornSteaks\n",
            "Retrieved 120 tweets for: FiveGuys\n",
            "Retrieved 120 tweets for: Applebees\n",
            "Retrieved 120 tweets for: DelTaco\n",
            "Retrieved 120 tweets for: PFChangs\n",
            "Retrieved 120 tweets for: BonefishGrill\n",
            "Retrieved 119 tweets for: Charleys\n",
            "Retrieved 120 tweets for: EinsteinBros\n",
            "Retrieved 120 tweets for: qdoba\n",
            "Retrieved 120 tweets for: torchystacos\n",
            "Retrieved 120 tweets for: raisingcanes\n",
            "Retrieved 120 tweets for: Cheesecake\n",
            "Retrieved 120 tweets for: WaffleHouse\n",
            "Retrieved 120 tweets for: CheckersRallys\n",
            "Retrieved 120 tweets for: SmoothieKing\n",
            "Retrieved 120 tweets for: CaptainDs\n",
            "Retrieved 119 tweets for: WhiteCastle\n",
            "Retrieved 120 tweets for: papamurphys\n",
            "Retrieved 120 tweets for: caferio\n",
            "Retrieved 120 tweets for: ChuysRestaurant\n",
            "Retrieved 118 tweets for: ChurchsChicken\n",
            "Retrieved 120 tweets for: IHOP\n",
            "Retrieved 120 tweets for: BaskinRobbins\n",
            "Retrieved 120 tweets for: TSmoothieCafe\n",
            "Retrieved 119 tweets for: MODPizza\n",
            "Retrieved 120 tweets for: calpizzakitchen\n",
            "Retrieved 119 tweets for: FreddysUSA\n",
            "Retrieved 120 tweets for: SteaknShake\n",
            "Retrieved 120 tweets for: tacojohns\n",
            "Retrieved 120 tweets for: Dickeys\n",
            "Retrieved 120 tweets for: krispykreme\n",
            "Retrieved 119 tweets for: ElPolloLoco\n",
            "Retrieved 119 tweets for: ColdStone\n",
            "Retrieved 120 tweets for: Whataburger\n",
            "Retrieved 120 tweets for: Hooters\n",
            "Retrieved 120 tweets for: Maggianos\n",
            "Retrieved 118 tweets for: hungryhowies\n",
            "Retrieved 120 tweets for: noodlescompany\n",
            "Retrieved 120 tweets for: Carrabbas\n",
            "Retrieved 120 tweets for: shakeshack\n",
            "Retrieved 120 tweets for: jimmyjohns\n",
            "Retrieved 120 tweets for: portilloshotdog\n",
            "Retrieved 120 tweets for: culvers\n",
            "Retrieved 119 tweets for: redrobinburgers\n",
            "Retrieved 120 tweets for: goldencorral\n",
            "Retrieved 120 tweets for: eatatjacks\n",
            "Retrieved 120 tweets for: McAlistersDeli\n",
            "Retrieved 120 tweets for: rubytuesday\n",
            "Retrieved 119 tweets for: BobEvansFarms\n",
            "Retrieved 120 tweets for: CHWinery\n",
            "Retrieved 119 tweets for: jasonsdeli\n",
            "Retrieved 120 tweets for: longjohnsilvers\n",
            "Retrieved 120 tweets for: TGIFridays\n",
            "Retrieved 120 tweets for: Potbelly\n",
            "Retrieved 120 tweets for: wingstop\n",
            "Retrieved 120 tweets for: JambaJuice\n",
            "Retrieved 120 tweets for: cheddarskitchen\n",
            "Retrieved 120 tweets for: CapitalGrille\n",
            "Retrieved 120 tweets for: EatAtPerkins\n",
            "Retrieved 117 tweets for: MillersAleHouse\n",
            "Retrieved 120 tweets for: JetsPizza\n",
            "Retrieved 117 tweets for: ZoesKitchen\n",
            "Retrieved 120 tweets for: peetscoffee\n",
            "Retrieved 120 tweets for: BlazePizza\n",
            "Retrieved 119 tweets for: bostonmarket\n",
            "Retrieved 120 tweets for: OCharleys\n",
            "Retrieved 120 tweets for: jerseymikes\n",
            "Retrieved 120 tweets for: bjsrestaurants\n",
            "Retrieved 120 tweets for: Bojangles\n",
            "Retrieved 120 tweets for: TimHortons\n",
            "Retrieved 120 tweets for: DennysDiner\n",
            "Retrieved 120 tweets for: Chilis\n",
            "Retrieved 119 tweets for: Outback\n",
            "Retrieved 120 tweets for: Moes_HQ\n",
            "Retrieved 120 tweets for: FirehouseSubs\n",
            "Retrieved 120 tweets for: DutchBros\n",
            "Retrieved 120 tweets for: RoundTablePizza\n",
            "Retrieved 119 tweets for: pollotropical\n",
            "Retrieved 120 tweets for: YardHouse\n",
            "Retrieved 120 tweets for: MellowMushroom\n",
            "Retrieved 119 tweets for: texasroadhouse\n",
            "Retrieved 120 tweets for: olivegarden\n",
            "Retrieved 120 tweets for: CrackerBarrel\n",
            "Retrieved 119 tweets for: PapaJohns\n",
            "Retrieved 120 tweets for: habitburger\n",
            "Retrieved 120 tweets for: Zaxbys\n",
            "Retrieved 120 tweets for: BWWings\n",
            "Retrieved 120 tweets for: MarcosPizza\n",
            "Retrieved 120 tweets for: McDonalds\n",
            "Retrieved 120 tweets for: CarlsJr\n"
          ]
        }
      ],
      "source": [
        "reply_tweetDf, userDf = get_api_data(twitter_handles, replies = True, numTweets = 80, tweetsPerPage = 40, print_status=True)\n",
        "noreply_tweetDf, userDf = get_api_data(twitter_handles, replies = False, numTweets = 120, tweetsPerPage = 60,print_status=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 102,
      "metadata": {},
      "outputs": [],
      "source": [
        "noreply_tweetDf[0].to_csv('noReplies.csv')\n",
        "reply_tweetDf.to_csv('replies.csv')\n",
        "userDf.to_csv('userData.csv')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyError",
          "evalue": "'userData'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_688/4123719630.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfUser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mapiData\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'userData'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdfUser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'userData.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyError\u001b[0m: 'userData'"
          ]
        }
      ],
      "source": [
        "# dfUser = apiData['userData']\n",
        "# dfUser.to_csv('userData.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# dfUser = apiData['userData']\n",
        "# for key, value in apiData\n",
        "# dfUser.to_csv('userData.csv')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "455-final-project.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
