{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-1.5.1-py3-none-win_amd64.whl (106.6 MB)\n",
      "Requirement already satisfied: scipy in c:\\users\\natha\\anaconda3\\lib\\site-packages (from xgboost) (1.7.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\natha\\anaconda3\\lib\\site-packages (from xgboost) (1.20.3)\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-1.5.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.linear_model as lm, pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = \"like_count_ln\"\n",
    "INCLUDE_SVR = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(url, drop=[]):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(url)\n",
    "    if len(drop) > 0:\n",
    "        for col in drop:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df\n",
    "\n",
    "def bin_groups(df, percent=.05):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            for group, count in df[col].value_counts().iteritems():\n",
    "                if count / len(df) < percent:\n",
    "                    df.loc[df[col] == group, col] = 'Other'\n",
    "    return df\n",
    "\n",
    "def drop_columns_missing_data(df, cutoff=.5):\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if df[col].isna().sum() / len(df) > cutoff:\n",
    "            df.drop(columns=[col], inplace=True)\n",
    "    return df\n",
    "\n",
    "def impute_mean(df):\n",
    "    from sklearn.impute import SimpleImputer\n",
    "    import pandas as pd, numpy as np\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "def impute_KNN(df):\n",
    "    from sklearn.impute import KNNImputer\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    df = pd.DataFrame(MinMaxScaler().fit_transform(df), columns = df.columns)\n",
    "    imp = KNNImputer(n_neighbors=5, weights=\"uniform\")\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "        \n",
    "def impute_reg(df):\n",
    "    from sklearn.experimental import enable_iterative_imputer\n",
    "    from sklearn.impute import IterativeImputer\n",
    "    import pandas as pd\n",
    "    for col in df:\n",
    "        if not pd.api.types.is_numeric_dtype(df[col]):\n",
    "            df = pd.get_dummies(df, columns=[col], drop_first=True)\n",
    "    imp = IterativeImputer(max_iter=10, random_state=12345)\n",
    "    df = pd.DataFrame(imp.fit_transform(df), columns=df.columns)\n",
    "    return df\n",
    "\n",
    "def fs_variance(df, label=\"\", p=0.8):\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    import pandas as pd\n",
    "\n",
    "    if label != \"\":\n",
    "        X = df.drop(columns=[label])\n",
    "    \n",
    "    sel = VarianceThreshold(threshold=(p * (1 - p)))\n",
    "    sel.fit(X)\n",
    "\n",
    "    # Add the label back in after removing poor features\n",
    "    return df[df.columns[sel.get_support(indices=True)]].join(df[label])\n",
    "\n",
    "def fit_crossvalidate_mlr(df, k, label, repeat=True):\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    import pandas as pd\n",
    "    from numpy import mean, std\n",
    "    X = df.drop(label,axis=1)\n",
    "    y = df[label]\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=5, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "    scores = cross_val_score(LinearRegression(), X, y, scoring='r2', cv=cv, n_jobs=-1)\n",
    "    print(f'Average R-squared:\\t{mean(scores)}')\n",
    "    return LinearRegression().fit(X, y)\n",
    "\n",
    "def dump_pickle(model, file_name):\n",
    "    import pickle\n",
    "    pickle.dump(model, open(file_name, \"wb\"))\n",
    "\n",
    "def load_pickle(file_name):\n",
    "    import pickle\n",
    "    model = pickle.load(open(file_name, \"rb\"))\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average R-squared:\t0.02536447581561319\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning and preparation pipeline\n",
    "df = get_data('noReplies-clean.csv')\n",
    "df = bin_groups(df)\n",
    "df = drop_columns_missing_data(df)\n",
    "df = impute_mean(df)\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df = fs_variance(df, label=LABEL, p=.5)\n",
    "model = fit_crossvalidate_mlr(df, 10, LABEL)\n",
    "\n",
    "# Deployment pipeline\n",
    "dump_pickle(model, 'saved_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>OLS</th>\n",
       "      <td>0.025416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ridge</th>\n",
       "      <td>0.025416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Bayesian</th>\n",
       "      <td>0.025416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lasso</th>\n",
       "      <td>0.025416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LARS</th>\n",
       "      <td>-0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Poisson</th>\n",
       "      <td>-0.000029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gamma</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Inverse</th>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          R-squared\n",
       "OLS        0.025416\n",
       "Ridge      0.025416\n",
       "Bayesian   0.025416\n",
       "Lasso      0.025416\n",
       "LARS      -0.000029\n",
       "Poisson   -0.000029\n",
       "Gamma           NaN\n",
       "Inverse         NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fit = {}    # Use this to store each of the fit metrics\n",
    "models = {} # Use this to store each of the models\n",
    "        \n",
    "# 1. LINEAR MODELS: assumes normal distribution, homoscedasticity, no multi-collinearity, independence, and no auto-correlation (some exceptions apply; some of these algorithms are better at handling violations of these assumptions)\n",
    "import sklearn.linear_model as lm, pandas as pd\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from numpy import mean\n",
    "\n",
    "# We will automate this later, just create\n",
    "X = df.drop(columns=[LABEL])\n",
    "y = df[LABEL]\n",
    "\n",
    "# Set up a standard cross_validation object to use for each algorithm\n",
    "cv = KFold(n_splits=5, random_state=12345, shuffle=True)\n",
    "\n",
    "# 1.1. Ordinary Least Squares Multiple Linear Regression\n",
    "model_ols = lm.LinearRegression()\n",
    "fit['OLS'] = mean(cross_val_score(model_ols, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['OLS'] = model_ols\n",
    "\n",
    "# 1.2. Ridge Regression: more robust to multi-collinearity\n",
    "model_rr = lm.Ridge(alpha=0.5) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['Ridge'] = mean(cross_val_score(model_rr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Ridge'] = model_rr\n",
    "\n",
    "# 1.3. Lasso Regression: better for sparse values like RetweetCount where most are zeros but a few have many retweets.\n",
    "model_lr = lm.Lasso(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['Lasso'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Lasso'] = model_lr\n",
    "\n",
    "# 1.4. Least Angle Regression: good when the number of features is greater than the number of samples\n",
    "model_llr = lm.LassoLars(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "fit['LARS'] = mean(cross_val_score(model_llr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['LARS'] = model_llr\n",
    "\n",
    "# 1.5. Bayesian Regression: probability based; allows regularization parameters, automatically tuned to data\n",
    "model_br = lm.BayesianRidge()\n",
    "fit['Bayesian'] = mean(cross_val_score(model_br, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Bayesian'] = model_br\n",
    "\n",
    "# 1.6. Generalized Linear Regression (Poisson): Good for non-normal distribution, count-based data, and a Poisson distribution\n",
    "model_pr = lm.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
    "fit['Poisson'] = mean(cross_val_score(model_pr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Poisson'] = model_pr\n",
    "\n",
    "# 1.7. Generalized Linear Regression (Gamma): Good for non-normal distribution, continuous data, and a Gamma distribution\n",
    "model_gr = lm.TweedieRegressor(power=2, link=\"log\") # Power=2 means this is a Gamma\n",
    "fit['Gamma'] = mean(cross_val_score(model_gr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Gamma'] = model_gr\n",
    "\n",
    "# 1.8. Generalized Linear Regression (Inverse Gamma): Good non-normal distribution, continuous data, and an inverse Gamma distribution\n",
    "model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
    "fit['Inverse'] = mean(cross_val_score(model_igr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Inverse'] = model_igr\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUPPORT VECTOR MACHINES: Ideal for noisy data with large gaps among values\n",
    "from sklearn import svm\n",
    "\n",
    "# 1.9. SVM: this is the default SVM, parameters can be modified to make this more accurate\n",
    "model_svm = svm.SVR()\n",
    "fit['SupportVM'] = mean(cross_val_score(model_svm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['SupportVM'] = model_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "BrokenProcessPool",
     "evalue": "A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\natha\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 407, in _process_worker\n    call_item = call_queue.get(block=True, timeout=timeout)\n  File \"C:\\Users\\natha\\anaconda3\\lib\\multiprocessing\\queues.py\", line 122, in get\n    return _ForkingPickler.loads(res)\nAttributeError: Can't get attribute 'new_block' on <module 'pandas.core.internals.blocks' from 'C:\\\\Users\\\\natha\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\core\\\\internals\\\\blocks.py'>\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17516/2182614994.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# 1.10. Linear SVM: Faster than SVM but only considers a linear model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel_lsvm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLinearSVR\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Linear SVM'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_lsvm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Linear SVM'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_lsvm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    443\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m     cv_results = cross_validate(estimator=estimator, X=X, y=y, groups=groups,\n\u001b[0m\u001b[0;32m    446\u001b[0m                                 \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                                 \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    248\u001b[0m     parallel = Parallel(n_jobs=n_jobs, verbose=verbose,\n\u001b[0;32m    249\u001b[0m                         pre_dispatch=pre_dispatch)\n\u001b[1;32m--> 250\u001b[1;33m     results = parallel(\n\u001b[0m\u001b[0;32m    251\u001b[0m         delayed(_fit_and_score)(\n\u001b[0;32m    252\u001b[0m             \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1055\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1056\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1057\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1058\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    933\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 935\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    936\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    937\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    443\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    444\u001b[0m                 \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 445\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    446\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    447\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    389\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 390\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    391\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    392\u001b[0m                 \u001b[1;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mBrokenProcessPool\u001b[0m: A task has failed to un-serialize. Please ensure that the arguments of the function are all picklable."
     ]
    }
   ],
   "source": [
    "# 1.10. Linear SVM: Faster than SVM but only considers a linear model\n",
    "model_lsvm = svm.LinearSVR()\n",
    "fit['Linear SVM'] = mean(cross_val_score(model_lsvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Linear SVM'] = model_lsvm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1.11. NuSVM: \n",
    "model_nusvm = svm.NuSVR()\n",
    "fit['NuSupportVM'] = mean(cross_val_score(model_nusvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['NuSupportVM'] = model_nusvm\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN: NEAREST NEIGHBORS REGRESSION\n",
    "from sklearn import neighbors\n",
    "\n",
    "# 1.12. KNeighborsRegressor: \n",
    "model_knnr = neighbors.KNeighborsRegressor(n_neighbors=10, weights='uniform')\n",
    "fit['KNNeighbors'] = mean(cross_val_score(model_knnr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['KNNeighbors'] = model_knnr\n",
    "\n",
    "# 1.13. KNeighborsRegressor: \n",
    "model_knnrd = neighbors.KNeighborsRegressor(n_neighbors=10, weights='distance')\n",
    "fit['KNNeighborsD'] = mean(cross_val_score(model_knnrd, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['KNNeighborsD'] = model_knnrd\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17516/3949049663.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m# 1.14. GaussianProcessRegressor:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel_gpr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgaussian_process\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGaussianProcessRegressor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDotProduct\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mWhiteKernel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mfit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GaussianP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_gpr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'r2'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GaussianP'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_gpr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'mean' is not defined"
     ]
    }
   ],
   "source": [
    "# GAUSSIAN PROCESS REGRESSION\n",
    "from sklearn import gaussian_process\n",
    "from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "\n",
    "# 1.14. GaussianProcessRegressor:\n",
    "model_gpr = gaussian_process.GaussianProcessRegressor(DotProduct() + WhiteKernel())\n",
    "fit['GaussianP'] = mean(cross_val_score(model_gpr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['GaussianP'] = model_gpr\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DECISION TREE MODELS: no assumptions about the data\n",
    "import sklearn.tree as tree\n",
    "import sklearn.ensemble as se\n",
    "\n",
    "# 1.15. Decision Tree Regression\n",
    "model_dt = tree.DecisionTreeRegressor(random_state=12345)\n",
    "fit['Dec Tree'] = mean(cross_val_score(model_dt, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Dec Tree'] = model_dt\n",
    "\n",
    "\n",
    "# DECISION TREE-BASED ENSEMBLE MODELS: great for minimizing overfitting, these are based on averaging many unique sub-samples and combining algorithms \n",
    "# 1.16. Decision Forrest\n",
    "model_df = se.RandomForestRegressor(random_state=12345)\n",
    "fit['Dec Forest'] = mean(cross_val_score(model_df, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Dec Forest'] = model_df\n",
    "\n",
    "# 1.17. ExtraTreesRegressor\n",
    "model_etr = se.ExtraTreesRegressor(random_state=12345)\n",
    "fit['Extra Trees'] = mean(cross_val_score(model_etr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Extra Trees'] = model_etr\n",
    "\n",
    "# 1.18. AdaBoostRegressor\n",
    "model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
    "fit['AdaBoost DT'] = mean(cross_val_score(model_abr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['AdaBoost DT'] = model_abr\n",
    "\n",
    "# 1.19. GradientBoostingRegressor\n",
    "model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
    "fit['Grad. Boost'] = mean(cross_val_score(model_gbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Grad. Boost'] = model_gbr\n",
    "\n",
    "# 1.20. HistGradientBoostingRegressor\n",
    "model_hgbr = se.HistGradientBoostingRegressor(random_state=12345)\n",
    "fit['HG Boost'] = mean(cross_val_score(model_hgbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['HG Boost'] = model_hgbr\n",
    "\n",
    "# 1.21. VotingRegressor: will combine other algorithms into an average; kind of cool\n",
    "model_vr = se.VotingRegressor(estimators=[('DT', model_dt), ('DF', model_df), ('ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "fit['Voting'] = mean(cross_val_score(model_vr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Voting'] = model_vr\n",
    "\n",
    "# 1.22. StackingRegressor\n",
    "from sklearn.linear_model import RidgeCV, LassoCV\n",
    "estimators = [('ridge', RidgeCV()), ('lasso', LassoCV(random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "model_sr = se.StackingRegressor(estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=12345))\n",
    "fit['Stacking'] = mean(cross_val_score(model_sr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['Stacking'] = model_sr\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "# 1.23. XGBRegressor\n",
    "model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['XGBoost'] = model_xgb\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEURAL-NETWORK MODELS: Based on deep learning methods\n",
    "import sklearn.neural_network as nn\n",
    "\n",
    "# 1.23. MLPRegressor\n",
    "model_nn = nn.MLPRegressor(max_iter=1000, random_state=12345) # Turn max_iter way up or down to get a more accurate result\n",
    "fit['NeuralNet'] = mean(cross_val_score(model_nn, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "models['NeuralNet'] = model_nn\n",
    "\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "# Sort and print the dictionary by greatest R squared to least\n",
    "df_fit = pd.DataFrame({'R-squared':fit})\n",
    "df_fit.sort_values(by=['R-squared'], ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_crossvalidate_reg(df, label, k=10, r=5, repeat=True):\n",
    "    import sklearn.linear_model as lm, pandas as pd, sklearn.ensemble as se\n",
    "    from sklearn.model_selection import KFold, RepeatedKFold, cross_val_score\n",
    "    from numpy import mean, std\n",
    "    from sklearn import svm\n",
    "    from sklearn import gaussian_process\n",
    "    from sklearn.gaussian_process.kernels import DotProduct, WhiteKernel\n",
    "    from xgboost import XGBRegressor\n",
    "\n",
    "    X = df.drop(columns=[label])\n",
    "    y = df[label]\n",
    "\n",
    "    if repeat:\n",
    "        cv = RepeatedKFold(n_splits=k, n_repeats=r, random_state=12345)\n",
    "    else:\n",
    "        cv = KFold(n_splits=k, random_state=12345, shuffle=True)\n",
    "    \n",
    "    fit = {}    # Use this to store each of the fit metrics\n",
    "    models = {} # Use this to store each of the models\n",
    "\n",
    "    # Create the model objects\n",
    "    model_ols = lm.LinearRegression()\n",
    "    model_rr = lm.Ridge(alpha=0.5) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_lr = lm.Lasso(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_llr = lm.LassoLars(alpha=0.1) # adjust this alpha parameter for better results (between 0 and 1)\n",
    "    model_br = lm.BayesianRidge()\n",
    "    model_pr = lm.TweedieRegressor(power=1, link=\"log\") # Power=1 means this is a Poisson\n",
    "    model_gr = lm.TweedieRegressor(power=2, link=\"log\") # Power=2 means this is a Gamma\n",
    "    model_igr = lm.TweedieRegressor(power=3) # Power=3 means this is an inverse Gamma\n",
    "    model_svm = svm.SVR()\n",
    "    model_lsvm = svm.LinearSVR()\n",
    "    model_nusvm = svm.NuSVR()\n",
    "    model_gpr = gaussian_process.GaussianProcessRegressor(DotProduct() + WhiteKernel())\n",
    "    model_df = se.RandomForestRegressor(random_state=12345)\n",
    "    model_etr = se.ExtraTreesRegressor(random_state=12345)\n",
    "    model_abr = se.AdaBoostRegressor(n_estimators=100, random_state=12345)\n",
    "    model_gbr = se.GradientBoostingRegressor(random_state=12345)\n",
    "    model_hgbr = se.HistGradientBoostingRegressor(random_state=12345)\n",
    "    model_vr = se.VotingRegressor(estimators=[('DF', model_df), ('ETR', model_etr), ('ABR', model_abr), ('GBR', model_gbr)])\n",
    "    estimators = [('ridge', lm.RidgeCV()), ('lasso', lm.LassoCV(random_state=42)), ('svr', svm.SVR(C=1, gamma=1e-6))]\n",
    "    model_sr = se.StackingRegressor(estimators=estimators, final_estimator=se.GradientBoostingRegressor(random_state=12345))\n",
    "    model_xgb = XGBRegressor(n_estimators=1000, max_depth=7, eta=0.1, subsample=0.7, colsample_bytree=0.8)\n",
    "    \n",
    "    # Fit a crss-validated R squared score and add it to the dict\n",
    "    fit['OLS'] = mean(cross_val_score(model_ols, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Ridge'] = mean(cross_val_score(model_rr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Lasso'] = mean(cross_val_score(model_lr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['LARS'] = mean(cross_val_score(model_llr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Bayesian'] = mean(cross_val_score(model_br, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Poisson'] = mean(cross_val_score(model_pr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Gamma'] = mean(cross_val_score(model_gr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Inverse'] = mean(cross_val_score(model_igr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['SupportVM'] = mean(cross_val_score(model_svm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Linear SVM'] = mean(cross_val_score(model_lsvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['NuSupportVM'] = mean(cross_val_score(model_nusvm, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['GaussianP'] = mean(cross_val_score(model_gpr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Dec Forest'] = mean(cross_val_score(model_df, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Extra Trees'] = mean(cross_val_score(model_etr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['AdaBoost DT'] = mean(cross_val_score(model_abr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Grad. Boost'] = mean(cross_val_score(model_gbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['HG Boost'] = mean(cross_val_score(model_hgbr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Voting'] = mean(cross_val_score(model_vr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['Stacking'] = mean(cross_val_score(model_sr, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "    fit['XGBoost'] = mean(cross_val_score(model_xgb, X, y, scoring='r2', cv=cv, n_jobs=-1))\n",
    "\n",
    "    # Add the model to another dict; make sure the keys have the same names as the list above\n",
    "    models['OLS'] = model_ols\n",
    "    models['Ridge'] = model_rr\n",
    "    models['Lasso'] = model_lr\n",
    "    models['LARS'] = model_llr\n",
    "    models['Bayesian'] = model_br\n",
    "    models['Poisson'] = model_pr\n",
    "    models['Gamma'] = model_gr\n",
    "    models['Inverse'] = model_igr\n",
    "    models['SupportVM'] = model_svm\n",
    "    models['Linear SVM'] = model_lsvm\n",
    "    models['NuSupportVM'] = model_nusvm\n",
    "    models['GaussianP'] = model_gpr\n",
    "    models['Dec Forest'] = model_df\n",
    "    models['Extra Trees'] = model_etr\n",
    "    models['AdaBoost DT'] = model_abr\n",
    "    models['Grad. Boost'] = model_gbr\n",
    "    models['HG Boost'] = model_hgbr\n",
    "    models['Voting'] = model_vr\n",
    "    models['Stacking'] = model_sr\n",
    "    models['XGBoost'] = model_xgb\n",
    "\n",
    "    # Add the fit dictionary to a new DataFrame, sort, extract the top row, use it to retrieve the model object from the models dictionary\n",
    "    df_fit = pd.DataFrame({'R-squared':fit})\n",
    "    df_fit.sort_values(by=['R-squared'], ascending=False, inplace=True)\n",
    "    best_model = df_fit.index[0]\n",
    "    print(df_fit)\n",
    "\n",
    "    return models[best_model].fit(X, y)\n",
    "\n",
    "# Now, see how this fits into the pipeline:---------------------------------------------\n",
    "# Data cleaning and preparation pipeline\n",
    "df = get_data('http://www.ishelp.info/data/housing_full.csv', ['Id'])\n",
    "df = bin_groups(df)\n",
    "df = drop_columns_missing_data(df)\n",
    "df = impute_mean(df)\n",
    "\n",
    "# Feature selection and modeling pipeline\n",
    "df = fs_variance(df, label=\"SalePrice\", p=.5)\n",
    "model = fit_crossvalidate_reg(df, 'SalePrice', 5, 2)\n",
    "\n",
    "# Deploy/store the model\n",
    "dump_pickle(model, 'best_reg_model.sav')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "model = load_pickle('best_reg_model.sav')\n",
    "print(pd.DataFrame({'Actual SalePrice':df.SalePrice, 'Predicted SalePrice':model.predict(df.drop(columns=['SalePrice']))}).head())"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d835b9d6547198496337f4d2de04abf3395832a8b4aee7b55cf3102d3ef3dae9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
