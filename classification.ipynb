{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featurize(df, label):\n",
    "  # Set label and features\n",
    "  df_copy = df.copy()\n",
    "  y = df_copy[label]\n",
    "  X = df_copy.assign(const=1)  # drop all categorical features and allow y-intercept to vary\n",
    "  X = X.drop(columns=[label])\n",
    "  return y, X\n",
    "\n",
    "def non_numeric_dtypes(df):\n",
    "  non_numeric = list(set(df.columns) - set(df.select_dtypes('number').columns))\n",
    "  print(non_numeric)\n",
    "  return non_numeric\n",
    "\n",
    "def group_by_counts(df, labels):\n",
    "  df_copy = df.copy()\n",
    "  df_copy = df_copy.groupby(labels).size.reset_index().rename(columns={0:'count'})\n",
    "  return df_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"noReplies-clean.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = featurize(df, \"IN_SCHOOL_FLAG\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=12345)\n",
    "clf = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "results = pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_dummies = pd.get_dummies(results['Actual'])\n",
    "y_pred_dummies = pd.get_dummies(results['Predicted'])\n",
    "\n",
    "# Accuracy  = (true positives + true negatives) / (total cases); ranges from 0 (worst) to 1 (best)\n",
    "print(f\"Accuracy:\\t{metrics.accuracy_score(y_test, y_pred)}\")\n",
    "\n",
    "# Precision = (true positives / (true positives + false positives))\n",
    "print(f\"Precision:\\t{metrics.precision_score(y_test_dummies[1], y_pred_dummies[1], labels=['0.0', '1.0'])}\")\n",
    "  \n",
    "# Recall    = (true positives / (true positives + false negatives)) \n",
    "print(f\"Recall:\\t\\t{metrics.recall_score(y_test_dummies[1], y_pred_dummies[1], labels=['0.0', '1.0'])}\")\n",
    "  \n",
    "# F1        = (2 * (precision * recall) / (precision + recall))\n",
    "print(f\"F1:\\t\\t{metrics.f1_score(y_test_dummies[1], y_pred_dummies[1], labels=['0.0', '1.0'])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
