{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installations\n",
    "\n",
    "%pip install pandas==1.1.5\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Further Installations\n",
    "\n",
    "%pip install pyLDAvis\n",
    "%pip install pyLDAvis.gensim\n",
    "%pip install bokeh\n",
    "%pip install gensim\n",
    "%pip install spacy\n",
    "%pip install logging\n",
    "%pip install wordcloud\n",
    "%pip install nltk\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install -U spacy\n",
    "%pip install -U seaborn\n",
    "%pip install translators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET \n",
    "import logging, warnings\n",
    "import spacy, gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from requests.utils import requote_uri\n",
    "from requests.utils import requote_uri\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splice(doc):\n",
    "    counts = 'numChars numCharsMinusSpacesAndPunctuation numWords numSentences numPunctuation'\n",
    "    speech = 'numNouns nounRatio numVerbs verbRatio numAdjectives adjectiveRatio numAdverbs adverbRatio'\n",
    "    immediacy = 'numPassiveVerbs passiveVerbRatio'\n",
    "    pronouns = 'firstPersonSingular firstPersonSingularRatio firstPersonPlural firstPersonPluralRatio secondPerson secondPersonRatio thirdPersonSingular thirdPersonSingularRatio thirdPersonPlural thirdPersonPluralRatio'\n",
    "    positive_self = 'iCanDoIt doKnow posSelfImage'\n",
    "    negative_self = 'iCantDoIt dontKnow negSelfImage'\n",
    "    influence = 'numImperatives suggestionPhrases inflexibility contradict totalDominance dominanceRatio numAgreement agreementRatio'\n",
    "    deference = \"askPermission seekGuidance totalSubmissiveness submissivenessRatio\"\n",
    "    affect = \"Imagery Pleasantness Activation\"\n",
    "    complexity = \"avgWordLength avgSentenceLength numSyllables avgSyllablesPerWord numWordsWith3OrMoreSyllables rateWordsWith3OrMoreSyllables numWordsWith6OrMoreChars rateWordsWith6OrMoreChars numWordsWith7OrMoreChars rateWordsWith7OrMoreChars LexicalDiversity complexityComposite\"\n",
    "    style = \"hedgeVerb hedgeConj hedgeAdj hedgeModal hedgeAll numDisfluencies disfluencyRatio numInterjections interjectionRatio numSpeculate speculateRatio Expressivity numIgnorance ignoranceRatio Pausality questionCount questionRatio hedgeUncertain\"\n",
    "    tense = 'pastTense pastTenseRatio presentTense presentTenseRatio'\n",
    "    sentiment = \"SWNpositivity SWNnegativity SWNobjectivity\"\n",
    "    readability = \"ARI FRE FKGL CLI LWRF FOG SMOG DALE LIX RIX FRY\"\n",
    "\n",
    "    url = f\"http://splice.cmi.arizona.edu/SPLICE/{doc}/{counts} {speech} {immediacy} {pronouns} {influence} {deference} {affect} {complexity} {style} {tense} {sentiment} {readability}\"\n",
    "    url_encoded = requote_uri(url)\n",
    "    req = requests.request(\"GET\", url_encoded)\n",
    "\n",
    "    results = []\n",
    "    root = ET.fromstring(req.text)\n",
    "    for child in list(root.getchildren()):\n",
    "        results.append(list([child.tag, child.text]))\n",
    "\n",
    "    return results\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words_extend=[], allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "  \"\"\"Remove Stopwords, Form Bigrams, Trigrams and perform Lemmatization\"\"\"\n",
    "  nltk.download('stopwords')\n",
    "  stop_words = stopwords.words('english')\n",
    "  stop_words.extend(['from', 'subject', 'co'])\n",
    "  stop_words.extend(stop_words_extend)\n",
    "\n",
    "  texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "  texts = [bigram_mod[doc] for doc in texts]\n",
    "  texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "  texts_out = []\n",
    "  nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])    # Load spacy, but we don't need the parser or NER (named entity extraction) modules\n",
    "\n",
    "  for sent in texts:\n",
    "    doc = nlp(\" \".join(sent)) \n",
    "    texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "\n",
    "  # remove stopwords once more after lemmatization\n",
    "  texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "  return texts_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def latent_dirichlet(data, topics=4, print_output=False, random_state=0, chunk_size=100, passes=5):\n",
    "    id2word = corpora.Dictionary(data)\n",
    "    corpus = [id2word.doc2bow(text) for text in data_ready]\n",
    "    lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=id2word, num_topics=topics, random_state=random_state, chunksize=chunk_size, passes=passes, per_word_topics=True)\n",
    "    ldatopics = lda_model.show_topics(formatted=False)\n",
    "    if print_output: pprint(lda_model.print_topics())\n",
    "    return lda_model, ldatopics, id2word, corpus\n",
    "\n",
    "def fit_lda (data, low=3, high=6, plot=False):\n",
    "    fit = pd.DataFrame(columns=['topics', 'perplexity', 'coherence'])\n",
    "    for n in range(low, high):\n",
    "        lda_model, ldatopics, id2word, corpus = latent_dirichlet(data, n)\n",
    "        coherence_model_lda = CoherenceModel(model=lda_model, texts=data_ready, dictionary=id2word, coherence='c_v')\n",
    "        fit.loc[str(n-low)] = [n, round(lda_model.log_perplexity(corpus), 3), round(coherence_model_lda.get_coherence(), 3)]\n",
    "    if plot:\n",
    "        import seaborn as sns\n",
    "        fit['diff'] = abs(fit.coherence - fit.perplexity)\n",
    "        sns.lineplot(x='topics', y='diff', data=fit, ci=None, marker='o');\n",
    "    return fit\n",
    "\n",
    "def score_topics(df, lda_model, id2word):\n",
    "    '''Returns scored DF. Pass in the DF used originally, the lda_model and the id2word from the lda function'''\n",
    "    df_topics = df.copy()\n",
    "\n",
    "    num_topics = len(lda_model.get_topics())\n",
    "    for col in range(num_topics):\n",
    "        df_topics[f'topic_{col + 1}'] = 0.0\n",
    "        \n",
    "    # Store the topic score and dominant topic\n",
    "    for i, words in enumerate(data_ready):\n",
    "        doc = lda_model[id2word.doc2bow(words)] # generate a corpus for this document set of words\n",
    "        \n",
    "        for j, score in enumerate(doc[0]): # for each document in the corpus\n",
    "        # Get the topic score and store it in the appropriate column\n",
    "            df_topics.iat[i, (len(df_topics.columns) - ((num_topics) - score[0]))] = score[1]\n",
    "\n",
    "    return df_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_keywords(lda_model):\n",
    "    from collections import Counter\n",
    "    import matplotlib.colors as mcolors\n",
    "    topics = lda_model.show_topics(formatted=False)\n",
    "    data_flat = [w for w_list in data_ready for w in w_list]\n",
    "    counter = Counter(data_flat)\n",
    "\n",
    "    out = []\n",
    "    for i, topic in topics:\n",
    "        for word, weight in topic:\n",
    "            out.append([word, i , weight, counter[word]])\n",
    "\n",
    "    df = pd.DataFrame(out, columns=['word', 'topic_id', 'importance', 'word_count'])        \n",
    "\n",
    "    # Plot Word Count and Weights of Topic Keywords\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16,10), sharey=True, dpi=160)\n",
    "    cols = [color for name, color in mcolors.TABLEAU_COLORS.items()]\n",
    "    for i, ax in enumerate(axes.flatten()):\n",
    "        ax.bar(x='word', height=\"word_count\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.5, alpha=0.3, label='Word Count')\n",
    "        ax_twin = ax.twinx()\n",
    "        ax_twin.bar(x='word', height=\"importance\", data=df.loc[df.topic_id==i, :], color=cols[i], width=0.2, label='Weights')\n",
    "        ax.set_ylabel('Word Count', color=cols[i])\n",
    "        # ax_twin.set_ylim(0, 0.030); ax.set_ylim(0, 3500)\n",
    "        ax.set_title('Topic: ' + str(i), color=cols[i], fontsize=16)\n",
    "        ax.tick_params(axis='y', left=False)\n",
    "        ax.set_xticklabels(df.loc[df.topic_id==i, 'word'], rotation=30, horizontalalignment= 'right')\n",
    "        ax.legend(loc='upper center'); ax_twin.legend(loc='upper right')\n",
    "\n",
    "    fig.tight_layout(w_pad=2)    \n",
    "    fig.suptitle('Word Count and Importance of Topic Keywords', fontsize=22, y=1.05)    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_topics(lda_model, corpus):\n",
    "    from sklearn.manifold import TSNE\n",
    "    from bokeh.plotting import figure, output_file, show\n",
    "    from bokeh.models import Label\n",
    "    from bokeh.io import output_notebook\n",
    "    import matplotlib.colors as mcolors\n",
    "    \n",
    "    # Get topic weights\n",
    "    topic_weights = []\n",
    "    for i, row_list in enumerate(lda_model[corpus]):\n",
    "        topic_weights.append([w for i, w in row_list[0]])\n",
    "    \n",
    "    # Array of topic weights    \n",
    "    arr = pd.DataFrame(topic_weights).fillna(0).values\n",
    "    \n",
    "    # Keep the well separated points (optional)\n",
    "    arr = arr[np.amax(arr, axis=1) > 0.35]\n",
    "    \n",
    "    # Dominant topic number in each doc\n",
    "    topic_num = np.argmax(arr, axis=1)\n",
    "    \n",
    "    # tSNE Dimension Reduction\n",
    "    tsne_model = TSNE(n_components=2, verbose=1, random_state=0, angle=.99, init='pca')\n",
    "    tsne_lda = tsne_model.fit_transform(arr)\n",
    "    \n",
    "    # Plot the Topic Clusters using Bokeh\n",
    "    output_notebook()\n",
    "    n_topics = 4\n",
    "    mycolors = np.array([color for name, color in mcolors.TABLEAU_COLORS.items()])\n",
    "    plot = figure(title=\"t-SNE Clustering of {} LDA Topics\".format(n_topics), \n",
    "                    plot_width=900, plot_height=700)\n",
    "    plot.scatter(x=tsne_lda[:,0], y=tsne_lda[:,1], color=mycolors[topic_num])\n",
    "    show(plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df = pd.read_csv(\"noReplies.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_scores(df):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    df_copy['sentiment_overall'] = 0.0\n",
    "    df_copy['sentiment_neg'] = 0.0\n",
    "    df_copy['sentiment_neu'] = 0.0\n",
    "    df_copy['sentiment_pos'] = 0.0\n",
    "\n",
    "    for row in df.itertuples():\n",
    "        sentiment = sia.polarity_scores(row[5])\n",
    "        df_copy.loc[row[0], 'sentiment_overall'] = sentiment['compound']\n",
    "        df_copy.loc[row[0], 'sentiment_neg'] = sentiment['neg']\n",
    "        df_copy.loc[row[0], 'sentiment_neu'] = sentiment['neu']\n",
    "        df_copy.loc[row[0], 'sentiment_pos'] = sentiment['pos']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 1060581\n"
     ]
    }
   ],
   "source": [
    "length = ''\n",
    "for i in range(len(data_words)):\n",
    "    for j in range(len(data_words[i])):\n",
    "        length += data_words[i][j]\n",
    "print(f'Corpus size: {str(len(length))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ready = process_words(data_words)  # processed Text Data!\n",
    "for tweet in data_ready[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = sentiment_scores(df)\n",
    "lda_model, ldatopics, id2word, corpus = latent_dirichlet(data_ready, print_output=True)\n",
    "df_scored = score_topics(df, lda_model, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_keywords(lda_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_topics(lda_model, corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scored.to_csv(\"noRepies-topics\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d835b9d6547198496337f4d2de04abf3395832a8b4aee7b55cf3102d3ef3dae9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
