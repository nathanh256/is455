{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Installations\n",
    "\n",
    "%pip install pandas==1.1.5\n",
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Further Installations\n",
    "\n",
    "%pip install pyLDAvis\n",
    "%pip install pyLDAvis.gensim\n",
    "%pip install bokeh\n",
    "%pip install gensim\n",
    "%pip install spacy\n",
    "%pip install logging\n",
    "%pip install wordcloud\n",
    "%pip install nltk\n",
    "%pip install -U pip setuptools wheel\n",
    "%pip install -U spacy\n",
    "%python -m spacy download en_core_web_sm\n",
    "%pip install -U seaborn\n",
    "%pip install translators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Imports\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "import sys\n",
    "import re\n",
    "import numpy as np\n",
    "import requests\n",
    "import xml\n",
    "import xml.etree.ElementTree as ET \n",
    "import logging, warnings\n",
    "import spacy, gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from requests.utils import requote_uri\n",
    "from requests.utils import requote_uri\n",
    "from pprint import pprint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splice(doc):\n",
    "    counts = 'numChars numCharsMinusSpacesAndPunctuation numWords numSentences numPunctuation'\n",
    "    speech = 'numNouns nounRatio numVerbs verbRatio numAdjectives adjectiveRatio numAdverbs adverbRatio'\n",
    "    immediacy = 'numPassiveVerbs passiveVerbRatio'\n",
    "    pronouns = 'firstPersonSingular firstPersonSingularRatio firstPersonPlural firstPersonPluralRatio secondPerson secondPersonRatio thirdPersonSingular thirdPersonSingularRatio thirdPersonPlural thirdPersonPluralRatio'\n",
    "    positive_self = 'iCanDoIt doKnow posSelfImage'\n",
    "    negative_self = 'iCantDoIt dontKnow negSelfImage'\n",
    "    influence = 'numImperatives suggestionPhrases inflexibility contradict totalDominance dominanceRatio numAgreement agreementRatio'\n",
    "    deference = \"askPermission seekGuidance totalSubmissiveness submissivenessRatio\"\n",
    "    affect = \"Imagery Pleasantness Activation\"\n",
    "    complexity = \"avgWordLength avgSentenceLength numSyllables avgSyllablesPerWord numWordsWith3OrMoreSyllables rateWordsWith3OrMoreSyllables numWordsWith6OrMoreChars rateWordsWith6OrMoreChars numWordsWith7OrMoreChars rateWordsWith7OrMoreChars LexicalDiversity complexityComposite\"\n",
    "    style = \"hedgeVerb hedgeConj hedgeAdj hedgeModal hedgeAll numDisfluencies disfluencyRatio numInterjections interjectionRatio numSpeculate speculateRatio Expressivity numIgnorance ignoranceRatio Pausality questionCount questionRatio hedgeUncertain\"\n",
    "    tense = 'pastTense pastTenseRatio presentTense presentTenseRatio'\n",
    "    sentiment = \"SWNpositivity SWNnegativity SWNobjectivity\"\n",
    "    readability = \"ARI FRE FKGL CLI LWRF FOG SMOG DALE LIX RIX FRY\"\n",
    "\n",
    "    url = f\"http://splice.cmi.arizona.edu/SPLICE/{doc}/{counts} {speech} {immediacy} {pronouns} {influence} {deference} {affect} {complexity} {style} {tense} {sentiment} {readability}\"\n",
    "    url_encoded = requote_uri(url)\n",
    "    req = requests.request(\"GET\", url_encoded)\n",
    "\n",
    "    results = []\n",
    "    root = ET.fromstring(req.text)\n",
    "    for child in list(root.getchildren()):\n",
    "        results.append(list([child.tag, child.text]))\n",
    "\n",
    "    return results\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sent in sentences:\n",
    "        sent = re.sub('\\S*@\\S*\\s?', '', sent)  # remove emails\n",
    "        sent = re.sub('\\s+', ' ', sent)  # remove newline chars\n",
    "        sent = re.sub(\"\\'\", \"\", sent)  # remove single quotes\n",
    "        sent = gensim.utils.simple_preprocess(str(sent), deacc=True) \n",
    "        yield(sent)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.extend(['from', 'subject', 'co'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_words(texts, stop_words=stop_words, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):\n",
    "    \"\"\"Remove Stopwords, Form Bigrams, Trigrams and perform Lemmatization\"\"\"\n",
    "    texts = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]\n",
    "    texts = [bigram_mod[doc] for doc in texts]\n",
    "    texts = [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "        \n",
    "    texts_out = []\n",
    "    nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])    # Load spacy, but we don't need the parser or NER (named entity extraction) modules\n",
    "        \n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent)) \n",
    "        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])\n",
    "        \n",
    "    # remove stopwords once more after lemmatization\n",
    "    texts_out = [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts_out]    \n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\natha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>handle</th>\n",
       "      <th>followers</th>\n",
       "      <th>text</th>\n",
       "      <th>lang</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>reply_count</th>\n",
       "      <th>like_count</th>\n",
       "      <th>...</th>\n",
       "      <th>contains_quote</th>\n",
       "      <th>is_reply</th>\n",
       "      <th>num_referenced_tweets</th>\n",
       "      <th>url_image</th>\n",
       "      <th>num_hashtags</th>\n",
       "      <th>text_first_hashtag</th>\n",
       "      <th>num_mentions</th>\n",
       "      <th>num_cashtags</th>\n",
       "      <th>num_polls</th>\n",
       "      <th>interaction_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1470814862871777287</td>\n",
       "      <td>Schlotzskys</td>\n",
       "      <td>19593</td>\n",
       "      <td>Challenging you to not bite your phone. https:...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001710</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1470095650150928385</td>\n",
       "      <td>Schlotzskys</td>\n",
       "      <td>19593</td>\n",
       "      <td>VOILA—this secret Santa is bringing Schlotzsky...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1469339638334697475</td>\n",
       "      <td>Schlotzskys</td>\n",
       "      <td>19593</td>\n",
       "      <td>No, no, no...you stay where you are. We'll com...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>15</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1469032106068254736</td>\n",
       "      <td>Schlotzskys</td>\n",
       "      <td>19593</td>\n",
       "      <td>Tag your better half. #SharingIsCaring https:/...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>34</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>SharingIsCaring</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.001837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1467906260418895881</td>\n",
       "      <td>Schlotzskys</td>\n",
       "      <td>19593</td>\n",
       "      <td>No holiday spread is complete without Cinnabon...</td>\n",
       "      <td>en</td>\n",
       "      <td>False</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000408</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                   id       handle  followers  \\\n",
       "0           0  1470814862871777287  Schlotzskys      19593   \n",
       "1           1  1470095650150928385  Schlotzskys      19593   \n",
       "2           2  1469339638334697475  Schlotzskys      19593   \n",
       "3           3  1469032106068254736  Schlotzskys      19593   \n",
       "4           4  1467906260418895881  Schlotzskys      19593   \n",
       "\n",
       "                                                text lang  possibly_sensitive  \\\n",
       "0  Challenging you to not bite your phone. https:...   en               False   \n",
       "1  VOILA—this secret Santa is bringing Schlotzsky...   en               False   \n",
       "2  No, no, no...you stay where you are. We'll com...   en               False   \n",
       "3  Tag your better half. #SharingIsCaring https:/...   en               False   \n",
       "4  No holiday spread is complete without Cinnabon...   en               False   \n",
       "\n",
       "   retweet_count  reply_count  like_count  ...  contains_quote is_reply  \\\n",
       "0              2            5          29  ...           False    False   \n",
       "1              2            0           1  ...           False    False   \n",
       "2              1            5          15  ...           False    False   \n",
       "3              2            5          34  ...           False    False   \n",
       "4              2            0           4  ...           False    False   \n",
       "\n",
       "  num_referenced_tweets url_image  num_hashtags  text_first_hashtag  \\\n",
       "0                     0       NaN             0                 NaN   \n",
       "1                     0       NaN             0                 NaN   \n",
       "2                     0       NaN             0                 NaN   \n",
       "3                     0       NaN             1     SharingIsCaring   \n",
       "4                     0       NaN             0                 NaN   \n",
       "\n",
       "   num_mentions  num_cashtags num_polls  interaction_score  \n",
       "0             0             0         0           0.001710  \n",
       "1             0             0         0           0.000332  \n",
       "2             0             0         0           0.000791  \n",
       "3             0             0         0           0.001837  \n",
       "4             0             0         0           0.000408  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "df = pd.read_csv(\"noReplies.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentiment_overall'] = 0.0\n",
    "df['sentiment_neg'] = 0.0\n",
    "df['sentiment_neu'] = 0.0\n",
    "df['sentiment_pos'] = 0.0\n",
    "\n",
    "for row in df.itertuples():\n",
    "    sentiment = sia.polarity_scores(row[5])\n",
    "    df.loc[row[0], 'sentiment_overall'] = sentiment['compound']\n",
    "    df.loc[row[0], 'sentiment_neg'] = sentiment['neg']\n",
    "    df.loc[row[0], 'sentiment_neu'] = sentiment['neu']\n",
    "    df.loc[row[0], 'sentiment_pos'] = sentiment['pos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['challenging', 'you', 'to', 'not', 'bite', 'your', 'phone', 'https', 'co', 'rcbzkwj']\n",
      "['voila', 'this', 'secret', 'santa', 'is', 'bringing', 'schlotzskys', 'gift', 'cards', 'to', 'the', 'party', 'https', 'co', 'ke', 'hqp', 'gej']\n",
      "['no', 'no', 'no', 'you', 'stay', 'where', 'you', 'are', 'well', 'come', 'to', 'you', 'free', 'delivery', 'on', 'saturday', 'and', 'sunday', 'https', 'co', 'lnazsp', 'wfn']\n",
      "['tag', 'your', 'better', 'half', 'sharingiscaring', 'https', 'co', 'nvvreuic']\n",
      "['no', 'holiday', 'spread', 'is', 'complete', 'without', 'cinnabon', 'add', 'to', 'your', 'catering', 'order', 'today', 'https', 'co', 'crbazf']\n"
     ]
    }
   ],
   "source": [
    "data = df.text.values.tolist()\n",
    "data_words = list(sent_to_words(data))\n",
    "for tweet in data_words[:5]: # print the first :n tweet word lists\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = ''\n",
    "for i in range(len(data_words)):\n",
    "    for j in range(len(data_words[i])):\n",
    "        length += data_words[i][j]\n",
    "print(f'Corpus size: {str(len(length))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) # higher threshold fewer phrases.\n",
    "trigram = gensim.models.Phrases(bigram[data_words], threshold=100)  \n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19368/2892185733.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdata_ready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_words\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# processed Text Data!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtweet\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_ready\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtweet\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19368/3089335613.py\u001b[0m in \u001b[0;36mprocess_words\u001b[1;34m(texts, stop_words, allowed_postags)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mtexts_out\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mnlp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mspacy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'en_core_web_sm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'parser'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ner'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m    \u001b[1;31m# Load spacy, but we don't need the parser or NER (named entity extraction) modules\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\__init__.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m     49\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mLanguage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mloaded\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m     \"\"\"\n\u001b[1;32m---> 51\u001b[1;33m     return util.load_model(\n\u001b[0m\u001b[0;32m     52\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdisable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdisable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexclude\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexclude\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     53\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\spacy\\util.py\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, exclude, config)\u001b[0m\n\u001b[0;32m    425\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    426\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE941\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfull\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mOLD_MODEL_SHORTCUTS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[index]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 427\u001b[1;33m     \u001b[1;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mE050\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    428\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'en_core_web_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "data_ready = process_words(data_words)  # processed Text Data!\n",
    "for tweet in data_ready[:5]:\n",
    "    print(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d835b9d6547198496337f4d2de04abf3395832a8b4aee7b55cf3102d3ef3dae9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
